{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using CapsNet and Keras to Identify Toxic Online Comments\n",
    "\n",
    "[Nick Walsh](https://twitter.com/thenickwalsh) \n",
    "-- Developer Evangelist, [Datmo](https://datmo.com)\n",
    "\n",
    "### Background\n",
    "\n",
    "The purpose of this notebook is to showcase the use of **Capsule Layers** / **CapsNet**, particularly in solving an NLP problem, where contextuality and superposition of data is important. You can read more CapsNets here [1](https://towardsdatascience.com/capsule-neural-networks-are-here-to-finally-recognize-spatial-relationships-693b7c99b12), [2](https://hackernoon.com/what-is-a-capsnet-or-capsule-network-2bfbe48769cc), [3](https://en.wikipedia.org/wiki/Capsule_neural_network). In this case, we'll be attempting to classify online comments as toxic or nontoxic. Toxic comments will also have a further classification, delineating between 1 of a few different \"styles\" of toxicity mentioned below. We'll be using **Datmo**'s [open source CLI](https://github.com/datmo/datmo) to help us get our environments sorted out and to track our experiment results and repository states along the way. \n",
    "\n",
    "The specific goal of this model, as mentioned in the original [Kaggle competition](https://www.kaggle.com/c/jigsaw-toxic-comment-classification-challenge) prompt, is to create a \"multi-headed model that’s capable of detecting different types of of toxicity like threats, obscenity, insults, and identity-based hate\". The __training and test datasets__ are comprised of comments from Wikipedia’s talk page edits.\n",
    "\n",
    "This notebook is a fork of the final [CapsNet+GRU kernel](https://www.kaggle.com/chongjiujjin/capsule-net-with-gru) submitted to the competition by [chongjiujjin](https://www.kaggle.com/chongjiujjin/).\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Training the Model\n",
    "\n",
    "We can easily start importing libraries without worrying about installing them on our system, since Datmo uses Docker under the hood to setup an environment for our notebook within a container."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['train.csv', 'crawl-300d-2M.vec', 'glove.840B.300d.txt', 'test.csv', 'test_labels.csv']\n"
     ]
    }
   ],
   "source": [
    "import numpy as np # linear algebra\n",
    "import pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n",
    "\n",
    "# Input data files are available in the \"input/\" directory.\n",
    "# For example, running this (by clicking run or pressing Shift+Enter) will list the files in the input directory\n",
    "\n",
    "import os\n",
    "from keras.layers import Dense,Input,LSTM,Bidirectional,Activation,Conv1D,GRU\n",
    "from keras.callbacks import Callback\n",
    "from keras.layers import Dropout,Embedding,GlobalMaxPooling1D, MaxPooling1D, Add, Flatten\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.layers import GlobalAveragePooling1D, GlobalMaxPooling1D, concatenate, SpatialDropout1D\n",
    "from keras import initializers, regularizers, constraints, optimizers, layers, callbacks\n",
    "from keras.callbacks import EarlyStopping,ModelCheckpoint\n",
    "from keras.models import Model\n",
    "from keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import roc_auc_score\n",
    "print(os.listdir(\"input\"))\n",
    "\n",
    "# Any results you write to the current directory are saved as output."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In NLP, an _embedding file_ contains dense quantified relationships between words, conveying more value than sparse techniques such as 'bag of words'. We can leverage these relationships in an _embedding layer_ later on in our model training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "_cell_guid": "79c7e3d0-c299-4dcb-8224-4455121ee9b0",
    "_uuid": "d629ff2d2480ee46fbb7e2d37f6b5fab8052498a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = 'input/glove.840B.300d.txt'\n",
    "\n",
    "train= pd.read_csv('input/train.csv')\n",
    "test = pd.read_csv('input/test.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "_cell_guid": "e8bd3575-f711-4ca6-a653-8ec1c74c0204",
    "_uuid": "cf43ac37cbd14d8baa088648c2275123550135d6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Extract the comment text from the input data (which contains indexing IDs that aren't meaningful to us)\n",
    "train[\"comment_text\"].fillna(\"fillna\")\n",
    "test[\"comment_text\"].fillna(\"fillna\")\n",
    "X_train = train[\"comment_text\"].str.lower()\n",
    "y_train = train[[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]].values\n",
    "\n",
    "X_test = test[\"comment_text\"].str.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "da409613-3688-4d2e-a072-f67dee02617b",
    "_uuid": "efad6a0ecd758a759f14287a69bfd9cafa8c8fb2",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define model hyperparameters\n",
    "max_features=100000\n",
    "maxlen=200\n",
    "embed_size=300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "3b0804b5-d64e-474e-8252-78daa4a4be62",
    "_uuid": "70c576f3b0afd3e779df184f788107fb51233424",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define optimization metric scoring function\n",
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: {:d} - score: {:.6f}\".format(epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "_cell_guid": "7a665c08-b4a9-4792-b40b-481b3da907e5",
    "_uuid": "b07e998ccedaf3aaaf4b4e67b207ad5490eb24f7",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenize input for use in model (NLP technique)\n",
    "tok=text.Tokenizer(num_words=max_features,lower=True)\n",
    "tok.fit_on_texts(list(X_train)+list(X_test))\n",
    "X_train=tok.texts_to_sequences(X_train)\n",
    "X_test=tok.texts_to_sequences(X_test)\n",
    "x_train=sequence.pad_sequences(X_train,maxlen=maxlen)\n",
    "x_test=sequence.pad_sequences(X_test,maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "9e57a7cb-c061-4361-bbe2-05c0486a3f18",
    "_uuid": "9488bc9d68dfd1fde1f99d23a9f1ed7b30ceb87f",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the GloVe embeddings\n",
    "embeddings_index = {}\n",
    "with open(EMBEDDING_FILE,encoding='utf8') as f:\n",
    "    for line in f:\n",
    "        values = line.rstrip().rsplit(' ')\n",
    "        word = values[0]\n",
    "        coefs = np.asarray(values[1:], dtype='float32')\n",
    "        embeddings_index[word] = coefs\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "e2490100-fc9c-4e46-ae84-7dfa65fcddba",
    "_uuid": "d56ad119931a971b2588355deb726a045764c9ad",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = tok.word_index\n",
    "#prepare embedding matrix\n",
    "num_words = min(max_features, len(word_index) + 1)\n",
    "embedding_matrix = np.zeros((num_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features:\n",
    "        continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "_cell_guid": "1a4a1cf3-7faf-4ee4-a72e-a258169778a5",
    "_uuid": "560d3faac051bbb95dae6f1bf7013d52b404533c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Define model architecture. This implementation is written entirely using Keras.\n",
    "\n",
    "from keras.layers import K, Activation\n",
    "from keras.engine import Layer\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Bidirectional, GRU, Flatten, SpatialDropout1D\n",
    "gru_len = 128\n",
    "Routings = 5\n",
    "Num_capsule = 10\n",
    "Dim_capsule = 16\n",
    "dropout_p = 0.25\n",
    "rate_drop_dense = 0.28\n",
    "\n",
    "def squash(x, axis=-1):\n",
    "    # s_squared_norm is really small\n",
    "    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
    "    # return scale * x\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale\n",
    "\n",
    "\n",
    "# A Capsule Implement with Pure Keras\n",
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    input1 = Input(shape=(maxlen,))\n",
    "    embed_layer = Embedding(max_features,\n",
    "                            embed_size,\n",
    "                            input_length=maxlen,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)(input1)\n",
    "    embed_layer = SpatialDropout1D(rate_drop_dense)(embed_layer)\n",
    "\n",
    "    x = Bidirectional(\n",
    "        GRU(gru_len, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, return_sequences=True))(\n",
    "        embed_layer)\n",
    "    capsule = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,\n",
    "                      share_weights=True)(x)\n",
    "    # output_capsule = Lambda(lambda x: K.sqrt(K.sum(K.square(x), 2)))(capsule)\n",
    "    capsule = Flatten()(capsule)\n",
    "    capsule = Dropout(dropout_p)(capsule)\n",
    "    output = Dense(6, activation='sigmoid')(capsule)\n",
    "    model = Model(inputs=input1, outputs=output)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "_cell_guid": "46df26aa-adcd-4b2c-8644-76a1e51df2bc",
    "_uuid": "19975febdf6a0bd3077d8a92da13bb433085ce80",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /opt/conda/lib/python3.6/site-packages/tensorflow/python/util/deprecation.py:381: calling conv1d (from tensorflow.python.ops.nn_ops) with data_format=NHWC is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "`NHWC` for data_format is deprecated, use `NWC` instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 200)               0         \n",
      "_________________________________________________________________\n",
      "embedding_1 (Embedding)      (None, 200, 300)          30000000  \n",
      "_________________________________________________________________\n",
      "spatial_dropout1d_1 (Spatial (None, 200, 300)          0         \n",
      "_________________________________________________________________\n",
      "bidirectional_1 (Bidirection (None, 200, 256)          329472    \n",
      "_________________________________________________________________\n",
      "capsule_1 (Capsule)          (None, 10, 16)            40960     \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 160)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 6)                 966       \n",
      "=================================================================\n",
      "Total params: 30,371,398\n",
      "Trainable params: 371,398\n",
      "Non-trainable params: 30,000,000\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.6/site-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n",
      "  FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# Instantiate architecture\n",
    "model = get_model()\n",
    "\n",
    "# Define training job parameters\n",
    "batch_size = 256\n",
    "epochs = 3\n",
    "\n",
    "#Define train-test data split for scoring model\n",
    "X_tra, X_val, y_tra, y_val = train_test_split(x_train, y_train, train_size=0.95, random_state=233)\n",
    "RocAuc = RocAucEvaluation(validation_data=(X_val, y_val), interval=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "_cell_guid": "e1962822-5dfb-4249-a714-ce95346150d4",
    "_uuid": "7956b05d34604689b7a10d56a61640091559eda2",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 151592 samples, validate on 7979 samples\n",
      "Epoch 1/1\n",
      "151552/151592 [============================>.] - ETA: 0s - loss: 0.0775 - acc: 0.9774\n",
      " ROC-AUC - epoch: 1 - score: 0.976220\n",
      "151592/151592 [==============================] - 691s 5ms/step - loss: 0.0775 - acc: 0.9774 - val_loss: 0.0525 - val_acc: 0.9805\n"
     ]
    }
   ],
   "source": [
    "# Fit model (will take some time)\n",
    "hist = model.fit(X_tra, y_tra, batch_size=batch_size, epochs=1, validation_data=(X_val, y_val),\n",
    "                 callbacks=[RocAuc], verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save weights from the model, allows prediction without retraining and sharing model with others.\n",
    "model.save_weights('best.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Let's create a snapshot of our model and training results!\n",
    "\n",
    "Great, we've been able to recreate the model on the training data! There are a few things we're going to want to do now.\n",
    "\n",
    "**Save your notebook!**\n",
    "\n",
    " * ` File --> Save and Checkpoint`\n",
    "\n",
    "\n",
    "**Create a snapshot** (in your root project folder)\n",
    "```bash\n",
    "$ datmo snapshot create -m \"Original capsulenet + GRU classifier\" --stats acc:0.9800 --config batch_size:256 --config epochs:3\n",
    "```\n",
    "\n",
    "**Visualize the snapshot we just took**\n",
    "\n",
    "```bash\n",
    "$ datmo snapshot ls\n",
    "+------------------------------------------+---------------------+------------------------------------------+---------------------+--------------------------------------+-------+\n",
    "|                    id                    |      created at     |                  config                  |        stats        |               message                | label |\n",
    "+------------------------------------------+---------------------+------------------------------------------+---------------------+--------------------------------------+-------+\n",
    "| 7a21e186c3e6e778455eb95f27b9cb94c41bd3c6 | 2018-06-08 01:59:21 | {u'epochs': u'3', u'batch_size': u'256'} | {u'acc': u'0.9800'} | Original capsulenet + GRU classifier |  None |\n",
    "+------------------------------------------+---------------------+------------------------------------------+---------------------+--------------------------------------+-------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating predictions for the Kaggle submission data\n",
    "\n",
    "Given a test set from the competition, we can use our model to create predictions for classifying the unlabeled  comments present in `test.csv`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "_cell_guid": "5c703574-cf76-495f-b800-1fc6c9384ded",
    "_uuid": "ddc5afff4b22841fb184e32cc4b19a2225dec451",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 243s 2ms/step\n"
     ]
    }
   ],
   "source": [
    "# Perform prediction on the Kaggle test set previously imported\n",
    "y_pred = model.predict(x_test, batch_size=1024, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.977612</td>\n",
       "      <td>0.394969</td>\n",
       "      <td>0.950550</td>\n",
       "      <td>0.099204</td>\n",
       "      <td>0.901001</td>\n",
       "      <td>0.216824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.004718</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>0.002531</td>\n",
       "      <td>0.002945</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.002568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.003458</td>\n",
       "      <td>0.002344</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.003776</td>\n",
       "      <td>0.002102</td>\n",
       "      <td>0.002878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.003513</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>0.002770</td>\n",
       "      <td>0.003654</td>\n",
       "      <td>0.002206</td>\n",
       "      <td>0.002168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.009126</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>0.003378</td>\n",
       "      <td>0.002320</td>\n",
       "      <td>0.003045</td>\n",
       "      <td>0.002157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.003855</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>0.002577</td>\n",
       "      <td>0.003744</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>0.002267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.017392</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>0.003295</td>\n",
       "      <td>0.002054</td>\n",
       "      <td>0.004815</td>\n",
       "      <td>0.002133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.633773</td>\n",
       "      <td>0.011423</td>\n",
       "      <td>0.094458</td>\n",
       "      <td>0.006996</td>\n",
       "      <td>0.210926</td>\n",
       "      <td>0.021639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.051208</td>\n",
       "      <td>0.002009</td>\n",
       "      <td>0.007375</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.003268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.004697</td>\n",
       "      <td>0.001848</td>\n",
       "      <td>0.002821</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.002094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.259536</td>\n",
       "      <td>0.004948</td>\n",
       "      <td>0.041486</td>\n",
       "      <td>0.003409</td>\n",
       "      <td>0.055571</td>\n",
       "      <td>0.007958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.209166</td>\n",
       "      <td>0.003758</td>\n",
       "      <td>0.028799</td>\n",
       "      <td>0.002944</td>\n",
       "      <td>0.045646</td>\n",
       "      <td>0.006615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.003685</td>\n",
       "      <td>0.002014</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>0.002735</td>\n",
       "      <td>0.002133</td>\n",
       "      <td>0.002445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.003383</td>\n",
       "      <td>0.002170</td>\n",
       "      <td>0.003089</td>\n",
       "      <td>0.002971</td>\n",
       "      <td>0.002201</td>\n",
       "      <td>0.002674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.002407</td>\n",
       "      <td>0.002799</td>\n",
       "      <td>0.003024</td>\n",
       "      <td>0.004613</td>\n",
       "      <td>0.002092</td>\n",
       "      <td>0.003135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       toxic  severe_toxic   obscene    threat    insult  identity_hate\n",
       "0   0.977612      0.394969  0.950550  0.099204  0.901001       0.216824\n",
       "1   0.004718      0.001969  0.002531  0.002945  0.002312       0.002568\n",
       "2   0.003458      0.002344  0.002853  0.003776  0.002102       0.002878\n",
       "3   0.003513      0.002292  0.002770  0.003654  0.002206       0.002168\n",
       "4   0.009126      0.001767  0.003378  0.002320  0.003045       0.002157\n",
       "5   0.003855      0.002208  0.002577  0.003744  0.002179       0.002267\n",
       "6   0.017392      0.001712  0.003295  0.002054  0.004815       0.002133\n",
       "7   0.633773      0.011423  0.094458  0.006996  0.210926       0.021639\n",
       "8   0.051208      0.002009  0.007375  0.002079  0.011300       0.003268\n",
       "9   0.004697      0.001848  0.002821  0.002583  0.002378       0.002094\n",
       "10  0.259536      0.004948  0.041486  0.003409  0.055571       0.007958\n",
       "11  0.209166      0.003758  0.028799  0.002944  0.045646       0.006615\n",
       "12  0.003685      0.002014  0.002778  0.002735  0.002133       0.002445\n",
       "13  0.003383      0.002170  0.003089  0.002971  0.002201       0.002674\n",
       "14  0.002407      0.002799  0.003024  0.004613  0.002092       0.003135"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Create a pandas dataframe from the model predictions (prev numpy array)\n",
    "prediction_df = pd.DataFrame(y_pred, columns=[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"])\n",
    "prediction_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "      <td>0.977612</td>\n",
       "      <td>0.394969</td>\n",
       "      <td>0.950550</td>\n",
       "      <td>0.099204</td>\n",
       "      <td>0.901001</td>\n",
       "      <td>0.216824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "      <td>0.004718</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>0.002531</td>\n",
       "      <td>0.002945</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.002568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "      <td>0.003458</td>\n",
       "      <td>0.002344</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.003776</td>\n",
       "      <td>0.002102</td>\n",
       "      <td>0.002878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "      <td>0.003513</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>0.002770</td>\n",
       "      <td>0.003654</td>\n",
       "      <td>0.002206</td>\n",
       "      <td>0.002168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "      <td>0.009126</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>0.003378</td>\n",
       "      <td>0.002320</td>\n",
       "      <td>0.003045</td>\n",
       "      <td>0.002157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001ea8717f6de06</td>\n",
       "      <td>Thank you for understanding. I think very high...</td>\n",
       "      <td>0.003855</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>0.002577</td>\n",
       "      <td>0.003744</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>0.002267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00024115d4cbde0f</td>\n",
       "      <td>Please do not add nonsense to Wikipedia. Such ...</td>\n",
       "      <td>0.017392</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>0.003295</td>\n",
       "      <td>0.002054</td>\n",
       "      <td>0.004815</td>\n",
       "      <td>0.002133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000247e83dcc1211</td>\n",
       "      <td>:Dear god this site is horrible.</td>\n",
       "      <td>0.633773</td>\n",
       "      <td>0.011423</td>\n",
       "      <td>0.094458</td>\n",
       "      <td>0.006996</td>\n",
       "      <td>0.210926</td>\n",
       "      <td>0.021639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00025358d4737918</td>\n",
       "      <td>\" \\n Only a fool can believe in such numbers. ...</td>\n",
       "      <td>0.051208</td>\n",
       "      <td>0.002009</td>\n",
       "      <td>0.007375</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.003268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00026d1092fe71cc</td>\n",
       "      <td>== Double Redirects == \\n\\n When fixing double...</td>\n",
       "      <td>0.004697</td>\n",
       "      <td>0.001848</td>\n",
       "      <td>0.002821</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.002094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0002eadc3b301559</td>\n",
       "      <td>I think its crap that the link to roggenbier i...</td>\n",
       "      <td>0.259536</td>\n",
       "      <td>0.004948</td>\n",
       "      <td>0.041486</td>\n",
       "      <td>0.003409</td>\n",
       "      <td>0.055571</td>\n",
       "      <td>0.007958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0002f87b16116a7f</td>\n",
       "      <td>\"::: Somebody will invariably try to add Relig...</td>\n",
       "      <td>0.209166</td>\n",
       "      <td>0.003758</td>\n",
       "      <td>0.028799</td>\n",
       "      <td>0.002944</td>\n",
       "      <td>0.045646</td>\n",
       "      <td>0.006615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0003806b11932181</td>\n",
       "      <td>, 25 February 2010 (UTC) \\n\\n :::Looking it ov...</td>\n",
       "      <td>0.003685</td>\n",
       "      <td>0.002014</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>0.002735</td>\n",
       "      <td>0.002133</td>\n",
       "      <td>0.002445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0003e1cccfd5a40a</td>\n",
       "      <td>\" \\n\\n It says it right there that it IS a typ...</td>\n",
       "      <td>0.003383</td>\n",
       "      <td>0.002170</td>\n",
       "      <td>0.003089</td>\n",
       "      <td>0.002971</td>\n",
       "      <td>0.002201</td>\n",
       "      <td>0.002674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00059ace3e3e9a53</td>\n",
       "      <td>\" \\n\\n == Before adding a new product to the l...</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>0.002799</td>\n",
       "      <td>0.003024</td>\n",
       "      <td>0.004613</td>\n",
       "      <td>0.002092</td>\n",
       "      <td>0.003135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                       comment_text  \\\n",
       "0   00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...   \n",
       "1   0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...   \n",
       "2   00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...   \n",
       "3   00017563c3f7919a  :If you have a look back at the source, the in...   \n",
       "4   00017695ad8997eb          I don't anonymously edit articles at all.   \n",
       "5   0001ea8717f6de06  Thank you for understanding. I think very high...   \n",
       "6   00024115d4cbde0f  Please do not add nonsense to Wikipedia. Such ...   \n",
       "7   000247e83dcc1211                   :Dear god this site is horrible.   \n",
       "8   00025358d4737918  \" \\n Only a fool can believe in such numbers. ...   \n",
       "9   00026d1092fe71cc  == Double Redirects == \\n\\n When fixing double...   \n",
       "10  0002eadc3b301559  I think its crap that the link to roggenbier i...   \n",
       "11  0002f87b16116a7f  \"::: Somebody will invariably try to add Relig...   \n",
       "12  0003806b11932181  , 25 February 2010 (UTC) \\n\\n :::Looking it ov...   \n",
       "13  0003e1cccfd5a40a  \" \\n\\n It says it right there that it IS a typ...   \n",
       "14  00059ace3e3e9a53  \" \\n\\n == Before adding a new product to the l...   \n",
       "\n",
       "       toxic  severe_toxic   obscene    threat    insult  identity_hate  \n",
       "0   0.977612      0.394969  0.950550  0.099204  0.901001       0.216824  \n",
       "1   0.004718      0.001969  0.002531  0.002945  0.002312       0.002568  \n",
       "2   0.003458      0.002344  0.002853  0.003776  0.002102       0.002878  \n",
       "3   0.003513      0.002292  0.002770  0.003654  0.002206       0.002168  \n",
       "4   0.009126      0.001767  0.003378  0.002320  0.003045       0.002157  \n",
       "5   0.003855      0.002208  0.002577  0.003744  0.002179       0.002267  \n",
       "6   0.017392      0.001712  0.003295  0.002054  0.004815       0.002133  \n",
       "7   0.633773      0.011423  0.094458  0.006996  0.210926       0.021639  \n",
       "8   0.051208      0.002009  0.007375  0.002079  0.011300       0.003268  \n",
       "9   0.004697      0.001848  0.002821  0.002583  0.002378       0.002094  \n",
       "10  0.259536      0.004948  0.041486  0.003409  0.055571       0.007958  \n",
       "11  0.209166      0.003758  0.028799  0.002944  0.045646       0.006615  \n",
       "12  0.003685      0.002014  0.002778  0.002735  0.002133       0.002445  \n",
       "13  0.003383      0.002170  0.003089  0.002971  0.002201       0.002674  \n",
       "14  0.002407      0.002799  0.003024  0.004613  0.002092       0.003135  "
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Let's combine the original comments with the model predictions so we can more easily make sense of the results.\n",
    "combined_df = test.join(prediction_df)\n",
    "combined_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "_cell_guid": "bffedb6f-f020-4ff8-b2f5-acd9c5957841",
    "_uuid": "f28a8748f9726bd1d3ef4dfc63fc97dbf54274d6",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Saving our dataframe to file\n",
    "combined_df.to_csv('kaggle_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### We've finished writing out our Kaggle results, let's take a snapshot.\n",
    "\n",
    "We can create another Datmo snapshot here to save the state of our environment, notebook, and files now that we're finished with our Kaggle results. This means we could always revert back to this snapshot's state should we decide to keep tinkering, or simply have a clean checkpoint for others to revert to should they want to reproduce our work up to this point.\n",
    "\n",
    "**Save your notebook!**\n",
    "\n",
    " * ` File --> Save and Checkpoint`\n",
    "\n",
    "\n",
    "**Create a snapshot** (in your root project folder)\n",
    "```bash\n",
    "$ datmo snapshot create -m \"CapsNet prediction on Kaggle test data\" --stats time:243s --config results_file:kaggle_results.csv \n",
    "```\n",
    "\n",
    "**Visualize the snapshot we just took** \n",
    "\n",
    "Notice, we can see the snapshot we just created, as well as the one we took immediately following training!\n",
    "\n",
    "```bash\n",
    "$ datmo snapshot ls\n",
    "+------------------------------------------+---------------------+------------------------------------------+---------------------+----------------------------------------+-------+\n",
    "|                    id                    |      created at     |                  config                  |        stats        |                message                 | label |\n",
    "+------------------------------------------+---------------------+------------------------------------------+---------------------+----------------------------------------+-------+\n",
    "| f398e5f6c50fc5a66f45bf77e81bcbcdeaafcdb8 | 2018-06-08 23:04:05 | {u'results_file': u'kaggle_results.csv'} |  {u'time': u'243s'} | CapsNet prediction on Kaggle test data |  None |\n",
    "| 7a21e186c3e6e778455eb95f27b9cb94c41bd3c6 | 2018-06-08 01:59:21 | {u'epochs': u'3', u'batch_size': u'256'} | {u'acc': u'0.9800'} |  Original capsulenet + GRU classifier  |  None |\n",
    "+------------------------------------------+---------------------+------------------------------------------+---------------------+----------------------------------------+-------+\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "# Using our saved model to predict on new data\n",
    "\n",
    "Although the Kaggle competition is finished, we can use our model to classify other comments and see what it thinks!\n",
    "\n",
    "Because the model weights are available (either saved by the user after training, or using the weights provided from the repository), we can perform this classification without needing to retrain the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Instantiate the model architecture\n",
    "\n",
    "This cell is only necessary if you are running the notebook without performing the training in the same session.\n",
    "Note: because we're using a custom layer (Capsule), we'll need to first redefine the architecture before\n",
    "loading in the weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras.layers import K, Activation\n",
    "from keras.engine import Layer\n",
    "from keras.layers import Dense, Input, Embedding, Dropout, Bidirectional, GRU, Flatten, SpatialDropout1D\n",
    "gru_len = 128\n",
    "Routings = 5\n",
    "Num_capsule = 10\n",
    "Dim_capsule = 16\n",
    "dropout_p = 0.25\n",
    "rate_drop_dense = 0.28\n",
    "\n",
    "def squash(x, axis=-1):\n",
    "    # s_squared_norm is really small\n",
    "    # s_squared_norm = K.sum(K.square(x), axis, keepdims=True) + K.epsilon()\n",
    "    # scale = K.sqrt(s_squared_norm)/ (0.5 + s_squared_norm)\n",
    "    # return scale * x\n",
    "    s_squared_norm = K.sum(K.square(x), axis, keepdims=True)\n",
    "    scale = K.sqrt(s_squared_norm + K.epsilon())\n",
    "    return x / scale\n",
    "\n",
    "\n",
    "# A Capsule Layer implementation with pure Keras\n",
    "class Capsule(Layer):\n",
    "    def __init__(self, num_capsule, dim_capsule, routings=3, kernel_size=(9, 1), share_weights=True,\n",
    "                 activation='default', **kwargs):\n",
    "        super(Capsule, self).__init__(**kwargs)\n",
    "        self.num_capsule = num_capsule\n",
    "        self.dim_capsule = dim_capsule\n",
    "        self.routings = routings\n",
    "        self.kernel_size = kernel_size\n",
    "        self.share_weights = share_weights\n",
    "        if activation == 'default':\n",
    "            self.activation = squash\n",
    "        else:\n",
    "            self.activation = Activation(activation)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(Capsule, self).build(input_shape)\n",
    "        input_dim_capsule = input_shape[-1]\n",
    "        if self.share_weights:\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(1, input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     # shape=self.kernel_size,\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "        else:\n",
    "            input_num_capsule = input_shape[-2]\n",
    "            self.W = self.add_weight(name='capsule_kernel',\n",
    "                                     shape=(input_num_capsule,\n",
    "                                            input_dim_capsule,\n",
    "                                            self.num_capsule * self.dim_capsule),\n",
    "                                     initializer='glorot_uniform',\n",
    "                                     trainable=True)\n",
    "\n",
    "    def call(self, u_vecs):\n",
    "        if self.share_weights:\n",
    "            u_hat_vecs = K.conv1d(u_vecs, self.W)\n",
    "        else:\n",
    "            u_hat_vecs = K.local_conv1d(u_vecs, self.W, [1], [1])\n",
    "\n",
    "        batch_size = K.shape(u_vecs)[0]\n",
    "        input_num_capsule = K.shape(u_vecs)[1]\n",
    "        u_hat_vecs = K.reshape(u_hat_vecs, (batch_size, input_num_capsule,\n",
    "                                            self.num_capsule, self.dim_capsule))\n",
    "        u_hat_vecs = K.permute_dimensions(u_hat_vecs, (0, 2, 1, 3))\n",
    "        # final u_hat_vecs.shape = [None, num_capsule, input_num_capsule, dim_capsule]\n",
    "\n",
    "        b = K.zeros_like(u_hat_vecs[:, :, :, 0])  # shape = [None, num_capsule, input_num_capsule]\n",
    "        for i in range(self.routings):\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))  # shape = [None, input_num_capsule, num_capsule]\n",
    "            c = K.softmax(b)\n",
    "            c = K.permute_dimensions(c, (0, 2, 1))\n",
    "            b = K.permute_dimensions(b, (0, 2, 1))\n",
    "            outputs = self.activation(K.batch_dot(c, u_hat_vecs, [2, 2]))\n",
    "            if i < self.routings - 1:\n",
    "                b = K.batch_dot(outputs, u_hat_vecs, [2, 3])\n",
    "\n",
    "        return outputs\n",
    "\n",
    "    def compute_output_shape(self, input_shape):\n",
    "        return (None, self.num_capsule, self.dim_capsule)\n",
    "\n",
    "\n",
    "def get_model():\n",
    "    input1 = Input(shape=(maxlen,))\n",
    "    embed_layer = Embedding(max_features,\n",
    "                            embed_size,\n",
    "                            input_length=maxlen,\n",
    "                            weights=[embedding_matrix],\n",
    "                            trainable=False)(input1)\n",
    "    embed_layer = SpatialDropout1D(rate_drop_dense)(embed_layer)\n",
    "\n",
    "    x = Bidirectional(\n",
    "        GRU(gru_len, activation='relu', dropout=dropout_p, recurrent_dropout=dropout_p, return_sequences=True))(\n",
    "        embed_layer)\n",
    "    capsule = Capsule(num_capsule=Num_capsule, dim_capsule=Dim_capsule, routings=Routings,\n",
    "                      share_weights=True)(x)\n",
    "    # output_capsule = Lambda(lambda x: K.sqrt(K.sum(K.square(x), 2)))(capsule)\n",
    "    capsule = Flatten()(capsule)\n",
    "    capsule = Dropout(dropout_p)(capsule)\n",
    "    output = Dense(6, activation='sigmoid')(capsule)\n",
    "    model = Model(inputs=input1, outputs=output)\n",
    "    model.compile(\n",
    "        loss='binary_crossentropy',\n",
    "        optimizer='adam',\n",
    "        metrics=['accuracy'])\n",
    "    model.summary()\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load the weights from disk for our model object\n",
    "model.load_weights('best.hdf5')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Predict on a handful of manually defined strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r",
      "3/3 [==============================] - 0s 12ms/step\n"
     ]
    }
   ],
   "source": [
    "X_test = [\"this is an innocent comment\", \"stfu baddie\", \"shit fuck piss\"] # create list of strings to test\n",
    "comment_df = pd.DataFrame(data=X_test,columns=[\"comment_text\"]) # create dataframe from this list for later\n",
    "\n",
    "# tokenize comments for use with the model\n",
    "X_test=tok.texts_to_sequences(X_test)\n",
    "x_test=sequence.pad_sequences(X_test,maxlen=maxlen)\n",
    "\n",
    "# perform prediction\n",
    "predictions = model.predict(x_test, batch_size=1024, verbose=1)[0:len(X_test)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>this is an innocent comment</td>\n",
       "      <td>0.041128</td>\n",
       "      <td>0.002055</td>\n",
       "      <td>0.004156</td>\n",
       "      <td>0.002384</td>\n",
       "      <td>0.008886</td>\n",
       "      <td>0.003515</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>stfu baddie</td>\n",
       "      <td>0.764418</td>\n",
       "      <td>0.024160</td>\n",
       "      <td>0.404104</td>\n",
       "      <td>0.008690</td>\n",
       "      <td>0.345152</td>\n",
       "      <td>0.035791</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>shit fuck piss</td>\n",
       "      <td>0.970915</td>\n",
       "      <td>0.411046</td>\n",
       "      <td>0.952534</td>\n",
       "      <td>0.080814</td>\n",
       "      <td>0.860406</td>\n",
       "      <td>0.196927</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  comment_text     toxic  severe_toxic   obscene    threat  \\\n",
       "0  this is an innocent comment  0.041128      0.002055  0.004156  0.002384   \n",
       "1                  stfu baddie  0.764418      0.024160  0.404104  0.008690   \n",
       "2               shit fuck piss  0.970915      0.411046  0.952534  0.080814   \n",
       "\n",
       "     insult  identity_hate  \n",
       "0  0.008886       0.003515  \n",
       "1  0.345152       0.035791  \n",
       "2  0.860406       0.196927  "
      ]
     },
     "execution_count": 81,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Visualize results\n",
    "prediction_df = pd.DataFrame(predictions, columns=[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"])\n",
    "combined_df = comment_df.join(prediction_df) # join the comment dataframe with the results dataframe\n",
    "combined_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optional: Write dataframe out to CSV\n",
    "combined_df.to_csv('manual_comment_results.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Predicting on a new CSV of comments\n",
    "\n",
    "Now that you understand how the code works on a handful of arbitrary strings, we can go a step further and perform prediction on a larger comment dataset loaded from a CSV file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Load in your own data\n",
    "new_comments_df = pd.read_csv('input/test.csv') # Replace 'test.csv' with your dataset\n",
    "X_test = test[\"comment_text\"].str.lower() # Replace \"comment_text\" with the label of the column containing your comments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Tokenizing your data for use with the model\n",
    "X_test=tok.texts_to_sequences(X_test)\n",
    "x_test=sequence.pad_sequences(X_test,maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "_cell_guid": "ad775280-23bf-49d0-aa5d-ede0bd7630f9",
    "_uuid": "522967eedb66526b24c220ebe6ec877ce62769bb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "153164/153164 [==============================] - 243s 2ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>00001cee341fdb12</td>\n",
       "      <td>Yo bitch Ja Rule is more succesful then you'll...</td>\n",
       "      <td>0.977612</td>\n",
       "      <td>0.394969</td>\n",
       "      <td>0.950550</td>\n",
       "      <td>0.099204</td>\n",
       "      <td>0.901001</td>\n",
       "      <td>0.216824</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0000247867823ef7</td>\n",
       "      <td>== From RfC == \\n\\n The title is fine as it is...</td>\n",
       "      <td>0.004718</td>\n",
       "      <td>0.001969</td>\n",
       "      <td>0.002531</td>\n",
       "      <td>0.002945</td>\n",
       "      <td>0.002312</td>\n",
       "      <td>0.002568</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>00013b17ad220c46</td>\n",
       "      <td>\" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...</td>\n",
       "      <td>0.003458</td>\n",
       "      <td>0.002344</td>\n",
       "      <td>0.002853</td>\n",
       "      <td>0.003776</td>\n",
       "      <td>0.002102</td>\n",
       "      <td>0.002878</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>00017563c3f7919a</td>\n",
       "      <td>:If you have a look back at the source, the in...</td>\n",
       "      <td>0.003513</td>\n",
       "      <td>0.002292</td>\n",
       "      <td>0.002770</td>\n",
       "      <td>0.003654</td>\n",
       "      <td>0.002206</td>\n",
       "      <td>0.002168</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>00017695ad8997eb</td>\n",
       "      <td>I don't anonymously edit articles at all.</td>\n",
       "      <td>0.009126</td>\n",
       "      <td>0.001767</td>\n",
       "      <td>0.003378</td>\n",
       "      <td>0.002320</td>\n",
       "      <td>0.003045</td>\n",
       "      <td>0.002157</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0001ea8717f6de06</td>\n",
       "      <td>Thank you for understanding. I think very high...</td>\n",
       "      <td>0.003855</td>\n",
       "      <td>0.002208</td>\n",
       "      <td>0.002577</td>\n",
       "      <td>0.003744</td>\n",
       "      <td>0.002179</td>\n",
       "      <td>0.002267</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>00024115d4cbde0f</td>\n",
       "      <td>Please do not add nonsense to Wikipedia. Such ...</td>\n",
       "      <td>0.017392</td>\n",
       "      <td>0.001712</td>\n",
       "      <td>0.003295</td>\n",
       "      <td>0.002054</td>\n",
       "      <td>0.004815</td>\n",
       "      <td>0.002133</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>000247e83dcc1211</td>\n",
       "      <td>:Dear god this site is horrible.</td>\n",
       "      <td>0.633773</td>\n",
       "      <td>0.011423</td>\n",
       "      <td>0.094458</td>\n",
       "      <td>0.006996</td>\n",
       "      <td>0.210926</td>\n",
       "      <td>0.021639</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>00025358d4737918</td>\n",
       "      <td>\" \\n Only a fool can believe in such numbers. ...</td>\n",
       "      <td>0.051208</td>\n",
       "      <td>0.002009</td>\n",
       "      <td>0.007375</td>\n",
       "      <td>0.002079</td>\n",
       "      <td>0.011300</td>\n",
       "      <td>0.003268</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>00026d1092fe71cc</td>\n",
       "      <td>== Double Redirects == \\n\\n When fixing double...</td>\n",
       "      <td>0.004697</td>\n",
       "      <td>0.001848</td>\n",
       "      <td>0.002821</td>\n",
       "      <td>0.002583</td>\n",
       "      <td>0.002378</td>\n",
       "      <td>0.002094</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0002eadc3b301559</td>\n",
       "      <td>I think its crap that the link to roggenbier i...</td>\n",
       "      <td>0.259536</td>\n",
       "      <td>0.004948</td>\n",
       "      <td>0.041486</td>\n",
       "      <td>0.003409</td>\n",
       "      <td>0.055571</td>\n",
       "      <td>0.007958</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0002f87b16116a7f</td>\n",
       "      <td>\"::: Somebody will invariably try to add Relig...</td>\n",
       "      <td>0.209166</td>\n",
       "      <td>0.003758</td>\n",
       "      <td>0.028799</td>\n",
       "      <td>0.002944</td>\n",
       "      <td>0.045646</td>\n",
       "      <td>0.006615</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0003806b11932181</td>\n",
       "      <td>, 25 February 2010 (UTC) \\n\\n :::Looking it ov...</td>\n",
       "      <td>0.003685</td>\n",
       "      <td>0.002014</td>\n",
       "      <td>0.002778</td>\n",
       "      <td>0.002735</td>\n",
       "      <td>0.002133</td>\n",
       "      <td>0.002445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0003e1cccfd5a40a</td>\n",
       "      <td>\" \\n\\n It says it right there that it IS a typ...</td>\n",
       "      <td>0.003383</td>\n",
       "      <td>0.002170</td>\n",
       "      <td>0.003089</td>\n",
       "      <td>0.002971</td>\n",
       "      <td>0.002201</td>\n",
       "      <td>0.002674</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>00059ace3e3e9a53</td>\n",
       "      <td>\" \\n\\n == Before adding a new product to the l...</td>\n",
       "      <td>0.002407</td>\n",
       "      <td>0.002799</td>\n",
       "      <td>0.003024</td>\n",
       "      <td>0.004613</td>\n",
       "      <td>0.002092</td>\n",
       "      <td>0.003135</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  id                                       comment_text  \\\n",
       "0   00001cee341fdb12  Yo bitch Ja Rule is more succesful then you'll...   \n",
       "1   0000247867823ef7  == From RfC == \\n\\n The title is fine as it is...   \n",
       "2   00013b17ad220c46  \" \\n\\n == Sources == \\n\\n * Zawe Ashton on Lap...   \n",
       "3   00017563c3f7919a  :If you have a look back at the source, the in...   \n",
       "4   00017695ad8997eb          I don't anonymously edit articles at all.   \n",
       "5   0001ea8717f6de06  Thank you for understanding. I think very high...   \n",
       "6   00024115d4cbde0f  Please do not add nonsense to Wikipedia. Such ...   \n",
       "7   000247e83dcc1211                   :Dear god this site is horrible.   \n",
       "8   00025358d4737918  \" \\n Only a fool can believe in such numbers. ...   \n",
       "9   00026d1092fe71cc  == Double Redirects == \\n\\n When fixing double...   \n",
       "10  0002eadc3b301559  I think its crap that the link to roggenbier i...   \n",
       "11  0002f87b16116a7f  \"::: Somebody will invariably try to add Relig...   \n",
       "12  0003806b11932181  , 25 February 2010 (UTC) \\n\\n :::Looking it ov...   \n",
       "13  0003e1cccfd5a40a  \" \\n\\n It says it right there that it IS a typ...   \n",
       "14  00059ace3e3e9a53  \" \\n\\n == Before adding a new product to the l...   \n",
       "\n",
       "       toxic  severe_toxic   obscene    threat    insult  identity_hate  \n",
       "0   0.977612      0.394969  0.950550  0.099204  0.901001       0.216824  \n",
       "1   0.004718      0.001969  0.002531  0.002945  0.002312       0.002568  \n",
       "2   0.003458      0.002344  0.002853  0.003776  0.002102       0.002878  \n",
       "3   0.003513      0.002292  0.002770  0.003654  0.002206       0.002168  \n",
       "4   0.009126      0.001767  0.003378  0.002320  0.003045       0.002157  \n",
       "5   0.003855      0.002208  0.002577  0.003744  0.002179       0.002267  \n",
       "6   0.017392      0.001712  0.003295  0.002054  0.004815       0.002133  \n",
       "7   0.633773      0.011423  0.094458  0.006996  0.210926       0.021639  \n",
       "8   0.051208      0.002009  0.007375  0.002079  0.011300       0.003268  \n",
       "9   0.004697      0.001848  0.002821  0.002583  0.002378       0.002094  \n",
       "10  0.259536      0.004948  0.041486  0.003409  0.055571       0.007958  \n",
       "11  0.209166      0.003758  0.028799  0.002944  0.045646       0.006615  \n",
       "12  0.003685      0.002014  0.002778  0.002735  0.002133       0.002445  \n",
       "13  0.003383      0.002170  0.003089  0.002971  0.002201       0.002674  \n",
       "14  0.002407      0.002799  0.003024  0.004613  0.002092       0.003135  "
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Perform predictions and concatenate data into one dataframe\n",
    "predictions = model.predict(x_test, batch_size=1024, verbose=1)[0:len(X_test)]\n",
    "prediction_df = pd.DataFrame(predictions, columns=[\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"])\n",
    "combined_df = new_comments_df.join(prediction_df) # join the comment dataframe with the results dataframe\n",
    "\n",
    "combined_df.head(15)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optional: Create an output folder to store your predictions\n",
    "\n",
    "outdir = './output'\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Optional: Create an output folder and write dataframe out to CSV\n",
    "\n",
    "outname = 'user_dataset_results.csv' # You can rename this file to whatever you'd like. Using the same output filename multiple times could cause overwriting, be careful!\n",
    "fullpath = os.path.join('./output', outname)  \n",
    "\n",
    "combined_df.to_csv(fullpath, index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
